{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33b3e8a",
   "metadata": {},
   "source": [
    "## 설정\n",
    "1. 설정/앱 vscode, miniconda3, python을 제거\n",
    "2. 사용자 계정 폴더 .conda .ipython .vscode .miniconda3 폴더와 condarc 파일을 제거\n",
    "3. c:\\Users\\사용자계정\\Appdata\\Roaming\\Code 제거\n",
    "\n",
    "- 기본 설치 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e13e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (1.1.0)\n",
      "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
      "  Downloading langchain_core-0.3.61-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain) (2.10.3)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.12.2)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.18-cp313-cp313-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp313-cp313-win_amd64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.2-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-openai) (1.77.0)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.9.0-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai) (0.4.6)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 17.2 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.61-py3-none-any.whl (438 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Downloading orjson-3.10.18-cp313-cp313-win_amd64.whl (134 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 22.3 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading zstandard-0.23.0-cp313-cp313-win_amd64.whl (495 kB)\n",
      "Downloading langchain_openai-0.3.18-py3-none-any.whl (63 kB)\n",
      "Downloading tiktoken-0.9.0-cp313-cp313-win_amd64.whl (894 kB)\n",
      "   ---------------------------------------- 0.0/894.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 894.7/894.7 kB 19.0 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.2-cp313-cp313-win_amd64.whl (296 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: zstandard, tenacity, orjson, jsonpointer, greenlet, tiktoken, SQLAlchemy, requests-toolbelt, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain\n",
      "\n",
      "   ----------------------------------------  0/14 [zstandard]\n",
      "   -- -------------------------------------  1/14 [tenacity]\n",
      "   ----- ----------------------------------  2/14 [orjson]\n",
      "   -------- -------------------------------  3/14 [jsonpointer]\n",
      "   ----------- ----------------------------  4/14 [greenlet]\n",
      "   ----------- ----------------------------  4/14 [greenlet]\n",
      "   ----------- ----------------------------  4/14 [greenlet]\n",
      "   ----------- ----------------------------  4/14 [greenlet]\n",
      "   ----------- ----------------------------  4/14 [greenlet]\n",
      "   ----------- ----------------------------  4/14 [greenlet]\n",
      "   -------------- -------------------------  5/14 [tiktoken]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   ----------------- ----------------------  6/14 [SQLAlchemy]\n",
      "   -------------------- -------------------  7/14 [requests-toolbelt]\n",
      "   -------------------- -------------------  7/14 [requests-toolbelt]\n",
      "   -------------------- -------------------  7/14 [requests-toolbelt]\n",
      "   ---------------------- -----------------  8/14 [jsonpatch]\n",
      "   ------------------------- --------------  9/14 [langsmith]\n",
      "   ------------------------- --------------  9/14 [langsmith]\n",
      "   ------------------------- --------------  9/14 [langsmith]\n",
      "   ------------------------- --------------  9/14 [langsmith]\n",
      "   ------------------------- --------------  9/14 [langsmith]\n",
      "   ------------------------- --------------  9/14 [langsmith]\n",
      "   ------------------------- --------------  9/14 [langsmith]\n",
      "   ------------------------- --------------  9/14 [langsmith]\n",
      "   ------------------------- --------------  9/14 [langsmith]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ---------------------------- ----------- 10/14 [langchain-core]\n",
      "   ------------------------------- -------- 11/14 [langchain-text-splitters]\n",
      "   ------------------------------- -------- 11/14 [langchain-text-splitters]\n",
      "   ---------------------------------- ----- 12/14 [langchain-openai]\n",
      "   ---------------------------------- ----- 12/14 [langchain-openai]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ------------------------------------- -- 13/14 [langchain]\n",
      "   ---------------------------------------- 14/14 [langchain]\n",
      "\n",
      "Successfully installed SQLAlchemy-2.0.41 greenlet-3.2.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.25 langchain-core-0.3.61 langchain-openai-0.3.18 langchain-text-splitters-0.3.8 langsmith-0.3.42 orjson-3.10.18 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.9.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef003886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-p'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() #.ebv 파일 내용을 환경변수로 로드\n",
    "import os\n",
    "os.getenv(\"OPENAI_API_KEY\")[:4] # 환경변수에서 OpenAI API 키를 가져옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13def61d",
   "metadata": {},
   "source": [
    "## Langchain의 핵심 컴포넌트 : 모델 호출 단계를 구성하는 추상화 요소를 제공\n",
    "- PromptTemplate : LLM에 보낼 입력 프롬프트\n",
    "- ChatOpenAI : openai의 GPT 모델 호출\n",
    "- Runnable : 실행 가능한 객체에 대한 공통 인터페이스 -> invoke() 메서드를 통해 입력 -> 출력 지원\n",
    "- StrOutPutParser : 문자열 출력 파서\n",
    "- 파이프로 연결 가능 ex) prompt | llm | strparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a875f3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피제품을 생산하는 회사 이름은 뭘로 하는게 좋을까?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"{product} 제품을 생산하는 회사 이름은 뭘로 하는게 좋을까?\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "formated_prompt = prompt.format(product = \"커피\") # PromptTemplate을 사용하여 템플릿에 값을 삽입\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e3c2e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 어떻게 도와드릴까요?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) # 러너블 객체. -> invoke() 메서드로 실행\n",
    "response = llm.invoke([('human','안녕')])\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17dc399e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 어떻게 도와드릴까요?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열 출력 파서 Runnable 객체 - invoke\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "parsed_text = parser.invoke(response) # 문자열을 파싱\n",
    "parsed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a0aa5",
   "metadata": {},
   "source": [
    "### Langchain Expression Language(LCEL) 단일 체인 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e311b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피 제품을 생산하는 회사 이름은 브랜드의 이미지와 제품의 특성을 잘 반영해야 합니다. 다음은 몇 가지 아이디어입니다:\n",
      "\n",
      "1. **커피의 정원** - 자연 친화적인 이미지를 강조\n",
      "2. **아침의 한 잔** - 아침에 커피를 즐기는 느낌을 전달\n",
      "3. **커피의 예술** - 고급스러운 이미지와 품질을 강조\n",
      "4. **향기로운 순간** - 커피의 향을 강조하는 이름\n",
      "5. **커피 이야기** - 각 커피의 배경과 스토리를 강조\n",
      "6. **커피 마법** - 특별한 경험을 제공하는 느낌\n",
      "7. **한 잔의 여유** - 편안함과 휴식을 강조\n",
      "8. **커피의 여정** - 다양한 원두와 블렌드를 탐험하는 느낌\n",
      "\n",
      "이름을 정할 때는 타겟 고객층과 브랜드의 비전을 고려하는 것이 중요합니다. 어떤 느낌이나 이미지를 전달하고 싶은지 생각해보세요!\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | parser # 파이프라인 형태로 연결, Runnable 객체 -> invoke() 메서드로 실행\n",
    "reuslt = chain.invoke({\"product\": \"coffee\"}) # 파이프라인 실행\n",
    "print(reuslt) # 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c35d58",
   "metadata": {},
   "source": [
    "### Langchain을 활용한 모델 사용, 비용 모니터링 및 캐싱 전략\n",
    "- GPT-4o-mini GPT-3.5-Turbo 비용이 60% 저렴\n",
    "- Langchain V0.3x 부터 openAI가 별도 패키지로 분리\n",
    "- 필요 패키지를 설치, langchai-openai 필요\n",
    "- 환경변수 변수 관리 패키지 python-dotevn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8eec236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (0.3.18)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: openai in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (1.77.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.61 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-openai) (0.3.61)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.61->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (2.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-community) (3.11.10)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain-community) (2.2.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\treze\\miniconda3\\envs\\llm\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 20.9 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "\n",
      "   ----- ---------------------------------- 1/8 [mypy-extensions]\n",
      "   ---------- ----------------------------- 2/8 [marshmallow]\n",
      "   ---------- ----------------------------- 2/8 [marshmallow]\n",
      "   --------------- ------------------------ 3/8 [httpx-sse]\n",
      "   -------------------- ------------------- 4/8 [typing-inspect]\n",
      "   ------------------------- -------------- 5/8 [pydantic-settings]\n",
      "   ------------------------- -------------- 5/8 [pydantic-settings]\n",
      "   ------------------------- -------------- 5/8 [pydantic-settings]\n",
      "   ------------------------------ --------- 6/8 [dataclasses-json]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ----------------------------------- ---- 7/8 [langchain-community]\n",
      "   ---------------------------------------- 8/8 [langchain-community]\n",
      "\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 typing-inspect-0.9.0 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai langchain-community python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05018bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() #.ebv 파일 내용을 환경변수로 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b96800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True) # 환경변수 덮어쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeae1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain은 자연어 처리(NLP) 모델을 활용하여 다양한 애플리케이션을 구축할 수 있도록 지원하는 프레임워크입니다.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7) # dotenv를 하면 자동으로 OPENAI_API_KEY 환경변수에 OpenAI API 키가 설정됨\n",
    "prompt = \"Langchain에 대해 한 문장으로 설명해줘\"\n",
    "result = llm.invoke(prompt)\n",
    "result.content # 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b546d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 18,\n",
       " 'output_tokens': 32,\n",
       " 'total_tokens': 50,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용량\n",
    "result.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ecc4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답1 죄송하지만, 현재  ...\n",
      "응답2 랭체인(RLangC ...\n",
      "총 토큰 수: 708\n",
      "프롬프트 토큰 수: 38\n",
      "응답 토큰 수: 670\n",
      "비용: 0.0004077\n"
     ]
    }
   ],
   "source": [
    "# 콜백함수를 통한 누적 토큰 추적(get_openai_callback)\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "with get_openai_callback() as cb:\n",
    "    # 첫번째 호출\n",
    "    res1 = llm.invoke(\"서울의 오늘 날씨는 어떤지 알려줘\")\n",
    "    print(\"응답1\", res1.content[:10],'...')\n",
    "    # 두번째 호출\n",
    "    res2 = llm.invoke(\"파이썬으로 랭체인 사용법을 알려줘\")\n",
    "    print(\"응답2\", res2.content[:10],'...')\n",
    "\n",
    "# 누적 토큰 사용량 출력 콜백 cb에는 블록 내 전체 토큰 사용량이 누적됨\n",
    "# 총 토큰 수\n",
    "print(\"총 토큰 수:\", cb.total_tokens)\n",
    "# 프롬프트 토큰 수\n",
    "print(\"프롬프트 토큰 수:\", cb.prompt_tokens)\n",
    "# 응답 토큰 수\n",
    "print(\"응답 토큰 수:\", cb.completion_tokens)\n",
    "# 비용 계산\n",
    "print(\"비용:\", cb.total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00cc63a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain의 LLM 응답 캐싱 (InMemory Cache, SQLite Cache)\n",
    "# 동일한 질문은 저장해뒀다가 응답에 사용 -> 비용 절약\n",
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "# InMemoryCache를 설정\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b92d9101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답1 : 물론이죠! 다음은 간단한 유머입니다:\n",
      "\n",
      "왜 컴퓨터는 바다에서 수영을 못할까요?\n",
      "\n",
      "너무 많은 \"버그\"가 있어서요! 🐛💻\n",
      "\n",
      "재미있으셨길 바랍니다!\n",
      "응답2 : 물론이죠! 다음은 간단한 유머입니다:\n",
      "\n",
      "왜 컴퓨터는 바다에서 수영을 못할까요?\n",
      "\n",
      "너무 많은 \"버그\"가 있어서요! 🐛💻\n",
      "\n",
      "재미있으셨길 바랍니다!\n"
     ]
    }
   ],
   "source": [
    "# 캐시 사용 전후를 비교, 같은 질문을 두번 호출\n",
    "query = \"재미있는 유머 하나 알려줘\"\n",
    "# 첫 번째 호출(캐시에 없으면 api 호출 발생)\n",
    "result1 = llm.invoke(query)\n",
    "print(f\"응답1 : {result1.content}\")\n",
    "\n",
    "# 두 번째 호출(동일한 쿼리 사용, 캐시를 확인 후 동일 질문이면 api를 호출하지 않음)\n",
    "result2 = llm.invoke(query)\n",
    "print(f\"응답2 : {result2.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba07152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 호출 시간: 5.151386737823486초\n",
      "두번째 호출 시간: 0.0010945796966552734초\n"
     ]
    }
   ],
   "source": [
    "# 실행시간 측정\n",
    "import time\n",
    "query = \"점심 메뉴 추천해줘\"\n",
    "# 첫번째 호출 시간\n",
    "start = time.time(); llm.invoke(query); end = time.time()\n",
    "print(f\"첫번째 호출 시간: {end - start}초\")\n",
    "# 두번째 호출 시간\n",
    "start = time.time(); llm.invoke(query); end = time.time()\n",
    "print(f\"두번째 호출 시간: {end - start}초\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite 캐시(디스크 공간 캐시)\n",
    "import os\n",
    "from langchain_community.cache import SQLiteCache\n",
    "# 기존 캐시 DB 파일이 있다면 삭제\n",
    "if os.path.exists(\".langchain.db\"):\n",
    "    os.remove(\".langchain.db\") # 실제 실습때는 삭제할 것.. 날아가면 안되니까.\n",
    "\n",
    "# SQLiteCache를 설정(저장한 경로에 DB 파일을 생성하고 저장)\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\")) # langchain.db 있으면 사용, 없으면 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f83244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답1 : 왜 컴퓨터는 바다에 빠지지 않을까요?\n",
      "\n",
      "왜냐하면 \"기다리기\"를 잘 못하니까요!\n",
      "첫번째 호출 시간: 1.687546730041504초\n",
      "응답2 : 왜 컴퓨터는 바다에 빠지지 않을까요?\n",
      "\n",
      "왜냐하면 \"기다리기\"를 잘 못하니까요!\n",
      "두번째 호출 시간: 0.010328531265258789초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 동일한 쿼리를 두번 호출해서 결과와 시간을 비교\n",
    "query = \"썰렁한 유머 하나 알려줘\"\n",
    "# 첫 번째 호출(캐시에 없으면 api 호출 발생)\n",
    "start = time.time(); result1 = llm.invoke(query); end = time.time()\n",
    "print(f\"응답1 : {result1.content}\")\n",
    "print(f\"첫번째 호출 시간: {end - start}초\")\n",
    "# 두 번째 호출(동일한 쿼리 사용, 캐시를 확인 후 동일 질문이면 api를 호출하지 않음)\n",
    "start = time.time(); result2 = llm.invoke(query); end = time.time()\n",
    "print(f\"응답2 : {result2.content}\")\n",
    "print(f\"두번째 호출 시간: {end - start}초\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239eae28",
   "metadata": {},
   "source": [
    "## Langchain 프롬프트 템플릿\n",
    "\n",
    "LLM 프롬프트를 동적으로 구성하고 재사용할 수 있도록 해 주는 도구\n",
    "- 단일입력 : 하나의 변수로 구성된 프롬프트 템플릿\n",
    "- 다중입력 : 둘 이상의 변수를 사용하는 템플릿\n",
    "- ChatPromptTemplate 역할 기반 프롬프트 : 시스템/사용자 역할별 프롬프트 구성 .from_message()\n",
    "- PartialPrompt Template 활용 : 프롬프트 일부를 미리 고정하고 부분 포맷팅을 사용 ex) 시스템 메시지는 고정\n",
    "- 프롬프트 출력 및 체인 실행 : LCEL(파이프라인 사용)\n",
    "- 프롬프트 작성 팁 : 주의사항 및 모범사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50f083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() #.env 파일 내용을 환경변수로 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c5a1196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 최고 수준의 마케팅 카피라이터 입니다. \n",
      "다음 제품에 대한 매력적인 홍보 문구를 작성해주세요:\n",
      "\n",
      "제품명 : 단백질 베이글\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 단일 프롬프트 사용\n",
    "from langchain.prompts import PromptTemplate\n",
    "# 템플릿 문자열 정의\n",
    "template_str = (\n",
    "    \"당신은 최고 수준의 마케팅 카피라이터 입니다. \\n\"\n",
    "    \"다음 제품에 대한 매력적인 홍보 문구를 작성해주세요:\\n\\n\"\n",
    "    \"제품명 : {product_name}\\n\"\n",
    ")\n",
    "\n",
    "# PromptTemplate 객체 생성\n",
    "product_prompt = PromptTemplate.from_template(template_str)\n",
    "# 템플릿에 값 삽입\n",
    "formatted_prompt = product_prompt.format(product_name=\"단백질 베이글\")\n",
    "# 프롬프트 출력\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cde363f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌟 **단백질 베이글: 건강과 맛을 동시에!** 🌟\n",
      "\n",
      "아침의 시작을 특별하게! 우리의 단백질 베이글은 풍부한 단백질과 함께 바삭한 식감, 그리고 고소한 맛을 자랑합니다. 운동 후 간편하게 즐길 수 있는 완벽한 스낵으로, 건강한 라이프스타일을 지향하는 당신에게 최적의 선택!\n",
      "\n",
      "🥯 **왜 단백질 베이글인가요?**\n",
      "- **영양 가득**: 하루의 에너지를 채워줄 단백질이 가득!\n",
      "- **맛의 혁신**: 전통 베이글의 풍미를 그대로, 건강은 더하고!\n",
      "- **간편한 식사**: 바쁜 아침에도, 언제 어디서나 쉽게 즐길 수 있어요!\n",
      "\n",
      "지금 바로 단백질 베이글로 건강한 변화를 시작하세요! 당신의 몸과 마음이 만족할 맛, 단백질 베이글과 함께라면 가능합니다. 💪✨\n",
      "\n",
      "**#단백질베이글 #건강한아침 #맛있는변화**\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 | LLM  -> invoke\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) # LLM 객체 생성\n",
    "# Runnable 객체로 연결\n",
    "chain = product_prompt | llm # 프롬프트와 LLM을 연결한 파이프라인\n",
    "result = chain.invoke({\"product_name\": \"단백질 베이글\"}) # invoke 메서드로 실행\n",
    "print(result.content) # 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f6008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 뉴스 기사 제목과 키워드 입니다.\n",
      "이 정보를 바탕으로 한 문단으로 구성된 간략한 요약문을 작성하세요.\n",
      "\n",
      "제목: AI 기술의 발전과 미래\n",
      "키워드: 인공지능, 머신러닝, 딥러닝\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다중입력\n",
    "# 다중입력 템플릿 문자열 정의\n",
    "multi_template_str = (\n",
    "    \"아래는 뉴스 기사 제목과 키워드 입니다.\\n\"\n",
    "    \"이 정보를 바탕으로 한 문단으로 구성된 간략한 요약문을 작성하세요.\\n\\n\"\n",
    "    \"제목: {title}\\n\"\n",
    "    \"키워드: {keywords}\\n\" \n",
    ")\n",
    "# 프롬프트 탬플릿 작성\n",
    "summary_prompt = PromptTemplate(template=multi_template_str, input_variables=[\"title\", \"keywords\"])\n",
    "# 포맷팅을 통해 프롬프트 값 확인\n",
    "sample_title = \"AI 기술의 발전과 미래\"\n",
    "sample_keywords = \"인공지능, 머신러닝, 딥러닝\"\n",
    "formatted_summary_prompt = summary_prompt.format(title = sample_title, keywords = sample_keywords)\n",
    "print(formatted_summary_prompt) # 프롬프트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aed1491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최근 인공지능(AI) 기술의 발전은 머신러닝과 딥러닝의 혁신적인 발전에 힘입어 가속화되고 있습니다. 이러한 기술들은 다양한 산업 분야에서 효율성을 높이고, 데이터 분석 및 예측 능력을 향상시키는 데 기여하고 있습니다. 앞으로 AI 기술은 더욱 진화하여 우리의 일상생활과 비즈니스 환경에 깊숙이 통합될 것으로 기대되며, 이는 새로운 기회와 도전 과제를 동시에 가져올 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# LCEL 출력\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) # LLM 객체 생성\n",
    "parser = StrOutputParser() # 문자열 출력 파서 객체 생성\n",
    "# Runnable 객체로 연결\n",
    "# summary_chain = summary_prompt | llm # 프롬프트와 LLM을 연결한 파이프라인\n",
    "result_summary = (summary_prompt | llm | parser).invoke({\"title\": sample_title, \"keywords\": sample_keywords}) # invoke 메서드로 실행\n",
    "print(result_summary) # 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17136723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate과 역할 기반 프롬프트\n",
    "# 시스템/사용자/어시스턴트 역할(role)\n",
    "# 시스템 메시지 : 모델의 동작을 지시\n",
    "# 사용자 메시지 : 실제 사용자의 입력\n",
    "# 어시스턴트 메시지 : 이전 모델이 응답한 내용이 있다면 대화 맥락 유지를 위해 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "044ddb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬에서 리스트를 정렬하는 방법은 매우 간단합니다. 주로 두 가지 방법을 사용합니다: `sort()` 메서드와 `sorted()` 함수입니다. 각각의 방법에 대해 설명해드릴게요.\n",
      "\n",
      "### 1. `sort()` 메서드\n",
      "`sort()` 메서드는 리스트 객체에 직접 적용되며, 리스트를 제자리에서 정렬합니다. 즉, 원래의 리스트가 변경됩니다.\n",
      "\n",
      "```python\n",
      "# 예시\n",
      "my_list = [3, 1, 4, 1, 5, 9]\n",
      "my_list.sort()  # 리스트를 정렬합니다.\n",
      "print(my_list)  # 출력: [1, 1, 3, 4, 5, 9]\n",
      "```\n",
      "\n",
      "- **주의**: `sort()` 메서드는 정렬된 리스트를 반환하지 않고, `None`을 반환합니다.\n",
      "\n",
      "### 2. `sorted()` 함수\n",
      "`sorted()` 함수는 리스트를 정렬하여 새로운 리스트를 반환합니다. 원래의 리스트는 변경되지 않습니다.\n",
      "\n",
      "```python\n",
      "# 예시\n",
      "my_list = [3, 1, 4, 1, 5, 9]\n",
      "sorted_list = sorted(my_list)  # 새로운 정렬된 리스트를 생성합니다.\n",
      "print(sorted_list)  # 출력: [1, 1, 3, 4, 5, 9]\n",
      "print(my_list)      # 원래 리스트는 변경되지 않음: [3, 1, 4, 1, 5, 9]\n",
      "```\n",
      "\n",
      "### 추가 옵션\n",
      "두 방법 모두 `reverse`와 `key` 매개변수를 사용할 수 있습니다.\n",
      "\n",
      "- **reverse**: `True`로 설정하면 내림차순으로 정렬합니다.\n",
      "- **key**: 정렬 기준을 지정할 수 있습니다.\n",
      "\n",
      "```python\n",
      "# 내림차순 정렬\n",
      "my_list = [3, 1, 4, 1, 5, 9]\n",
      "my_list.sort(reverse=True)\n",
      "print(my_list)  # 출력: [9, 5, 4, 3, 1, 1]\n",
      "\n",
      "# key를 사용한 정렬 (문자열 길이 기준)\n",
      "words = [\"apple\", \"banana\", \"cherry\", \"date\"]\n",
      "sorted_words = sorted(words, key=len)\n",
      "print(sorted_words)  # 출력: ['date', 'apple', 'banana', 'cherry']\n",
      "```\n",
      "\n",
      "이렇게 리스트를 정렬하는 방법을 사용하면 됩니다! 궁금한 점이 더 있으면 언제든지 질문해 주세요.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system_message = (\n",
    "    \"당신은 python 분야의 뛰어난 전문가이자 조언가입니다. \\n\"\n",
    "    \"사용자의 프로그래밍 질문에 대해 친절하고 이해하기 쉽게 답변해주세요.\\n\"\n",
    ")\n",
    "user_message = \"{question}\" # 사용자 질문을 입력받는 변수\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message), # 시스템 메시지\n",
    "    (\"human\", user_message) # 사용자 메시지\n",
    "])\n",
    "# 템플릿을 이용해서 문장을 완성해 보세요.\n",
    "formatted_chat_prompt = chat_prompt.format(question=\"파이썬에서 리스트를 정렬하는 방법은 무엇인가요?\")\n",
    "\n",
    "# 파이프라인을 이용해서 llm 호출 및 파싱\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) # LLM 객체 생성\n",
    "parser = StrOutputParser() # 문자열 출력 파서 객체 생성\n",
    "result_chat = (chat_prompt | llm | parser).invoke({\"question\": formatted_chat_prompt}) # invoke 메서드로 실행\n",
    "print(result_chat) # 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fc0e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PartialPromptTemplate : 템플릿의 일부를 부분적으로 채운 새로운 템플릿 생성성\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "role_system_template = \"당신은 {role} 분야의 전문 지식인입니다. 가능한 자세히 답변해 주세요.\"\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(role_system_template) # 부분 템플릿 생성\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\"{question}\") # 사용자 메시지 템플릿 생성\n",
    "\n",
    "# ChatPromptTemplate을 사용하여 역할 기반 프롬프트 생성\n",
    "base_chat_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])\n",
    "\n",
    "partial_chat_prompt = base_chat_prompt.partial(role = \"주식투자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89be004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='당신은 주식투자 분야의 전문 지식인입니다. 가능한 자세히 답변해 주세요.', additional_kwargs={}, response_metadata={}), HumanMessage(content='현재 2025년 5월 시장 상황에서 삼성전자 주식 전망은?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# partial로 생성된 프롬프트에 질문만 채워 프롬프트 구성\n",
    "sample_question = \"현재 2025년 5월 시장 상황에서 삼성전자 주식 전망은?\"\n",
    "message = partial_chat_prompt.format_messages(question=sample_question)\n",
    "print(message) # 프롬프트 메시지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb6101a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025년 5월 현재 삼성전자 주식의 전망을 평가하기 위해서는 여러 가지 요소를 고려해야 합니다. 다음은 삼성전자의 주식 전망에 영향을 미칠 수 있는 주요 요인들입니다.\n",
      "\n",
      "### 1. **업종 및 시장 동향**\n",
      "   - **반도체 산업**: 삼성전자는 세계 최대의 반도체 제조업체 중 하나로, 메모리 반도체(DRAM, NAND)와 시스템 반도체 분야에서 강력한 입지를 가지고 있습니다. 2025년에는 AI, IoT, 5G 등 다양한 기술의 발전으로 반도체 수요가 증가할 것으로 예상됩니다. 특히, AI와 관련된 반도체 수요는 급증할 가능성이 높습니다.\n",
      "   - **스마트폰 및 가전제품**: 삼성전자는 스마트폰과 가전제품에서도 강력한 브랜드를 보유하고 있습니다. 5G와 AI 기술이 접목된 새로운 제품들이 시장에 출시되면서 소비자 수요가 증가할 것으로 보입니다.\n",
      "\n",
      "### 2. **재무 성과**\n",
      "   - 삼성전자의 최근 분기 실적과 연간 실적을 분석해야 합니다. 매출 성장률, 순이익, 영업이익률 등 주요 재무 지표가 긍정적이라면 주가에 긍정적인 영향을 미칠 것입니다.\n",
      "   - 또한, 배당금 지급 정책도 투자자에게 중요한 요소입니다. 안정적인 배당금 지급은 주가에 긍정적인 영향을 미칠 수 있습니다.\n",
      "\n",
      "### 3. **글로벌 경제 상황**\n",
      "   - 글로벌 경제의 성장률, 금리, 인플레이션 등은 삼성전자의 주가에 큰 영향을 미칠 수 있습니다. 예를 들어, 금리가 상승하면 기업의 자금 조달 비용이 증가하고, 소비자 지출이 줄어들 수 있습니다.\n",
      "   - 또한, 미중 무역 갈등이나 글로벌 공급망 문제 등도 삼성전자의 운영에 영향을 미칠 수 있습니다.\n",
      "\n",
      "### 4. **경쟁 상황**\n",
      "   - 삼성전자는 TSMC, 인텔, SK hynix 등과 같은 경쟁업체와의 경쟁에서 어떻게 대응하고 있는지 살펴봐야 합니다. 기술 혁신, 가격 경쟁력, 생산능력 등이 중요한 요소입니다.\n",
      "\n",
      "### 5. **기술 혁신 및 연구개발**\n",
      "   - 삼성전자는 지속적으로 연구개발에 투자하고 있으며, 새로운 기술을 선보이는 것이 중요합니다. 특히, 반도체 분야에서의 기술 혁신은 장기적인 성장에 큰 영향을 미칠 수 있습니다.\n",
      "\n",
      "### 6. **정치적 및 사회적 요인**\n",
      "   - 한국의 정치적 안정성, 규제 환경, 그리고 글로벌 정치적 상황도 삼성전자의 주가에 영향을 미칠 수 있습니다. 예를 들어, 한국 정부의 반도체 산업 지원 정책은 긍정적인 영향을 미칠 수 있습니다.\n",
      "\n",
      "### 결론\n",
      "2025년 5월 현재 삼성전자의 주식 전망은 긍정적일 수 있지만, 위에서 언급한 다양한 요인들을 종합적으로 고려해야 합니다. 특히 반도체 산업의 성장 가능성과 삼성전자의 기술 혁신이 주가에 긍정적인 영향을 미칠 것으로 예상됩니다. 그러나 글로벌 경제 상황과 경쟁 환경도 주의 깊게 살펴봐야 합니다. 투자 결정을 내리기 전에 최신 정보와 분석을 참고하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "# LCEL 이용하여 LLM 호출 및 파싱\n",
    "result_partial = (partial_chat_prompt | llm | parser).invoke(message) # invoke 메서드로 실행\n",
    "print(result_partial) # 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c50d46",
   "metadata": {},
   "source": [
    "### Langchain Memory\n",
    "- 이전 대화 내용을 기억해서 문맥을 유지하는 역할\n",
    "- Langchain 0.3x부터는 LCEL 기반으로 체인을 구성\n",
    "- RunnableWithMessageHistory, ChatMessageHistory등의 컴포넌트를 활용해서 세션별 대화 기록을 관리\n",
    "- 대화가 장기화될 경우 요약 메모리를 도입해서 과거 대화를 LLM으로 요약하고 축약된 형태로 저장해서 프롬프트의 길이 문제를 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7531b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langchain langchain-openai langchain-community python-dotenv langchain_redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f81fbd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() #.env 파일 내용을 환경변수로 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fac36433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: 안녕하세요 제 이름은 홍길동입니다.\n",
      "ai: 안녕하세요 홍길동님, 무엇을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "# 메모리 객체 생성\n",
    "history = InMemoryChatMessageHistory()\n",
    "history.add_user_message(\"안녕하세요 제 이름은 홍길동입니다.\") # 사용자 메시지 추가\n",
    "history.add_ai_message(\"안녕하세요 홍길동님, 무엇을 도와드릴까요?\") # AI 메시지 추가\n",
    "# 현재까지의 대화 내용 확인\n",
    "for msg in history.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\") # 메시지 타입과 내용 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "872ed52a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error 10061 connecting to localhost:6379. 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\connection.py:378\u001b[39m, in \u001b[36mAbstractConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     sock = \u001b[38;5;28mself\u001b[39m.retry.call_with_retry(\n\u001b[32m    379\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._connect(), \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m.disconnect(error)\n\u001b[32m    380\u001b[39m     )\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\retry.py:62\u001b[39m, in \u001b[36mRetry.call_with_retry\u001b[39m\u001b[34m(self, do, fail)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m do()\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m._supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\connection.py:379\u001b[39m, in \u001b[36mAbstractConnection.connect.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    378\u001b[39m     sock = \u001b[38;5;28mself\u001b[39m.retry.call_with_retry(\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._connect(), \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m.disconnect(error)\n\u001b[32m    380\u001b[39m     )\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\connection.py:764\u001b[39m, in \u001b[36mConnection._connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msocket.getaddrinfo returned an empty list\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\connection.py:752\u001b[39m, in \u001b[36mConnection._connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;66;03m# connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m sock.connect(socket_address)\n\u001b[32m    754\u001b[39m \u001b[38;5;66;03m# set the socket_timeout now that we're connected\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m REDIS_URL = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mREDIS_URL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mredis://localhost:6379\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# 환경변수에서 Redis 호스트 정보 가져오기\u001b[39;00m\n\u001b[32m      5\u001b[39m session_id = \u001b[33m'\u001b[39m\u001b[33muser_123\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m history = RedisChatMessageHistory(session_id=session_id, redis_url=REDIS_URL)\n\u001b[32m      7\u001b[39m history.add_user_message(\u001b[33m\"\u001b[39m\u001b[33m안녕하세요 제 이름은 홍길동입니다.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# 사용자 메시지 추가\u001b[39;00m\n\u001b[32m      8\u001b[39m history.add_ai_message(\u001b[33m\"\u001b[39m\u001b[33m안녕하세요 홍길동님, 무엇을 도와드릴까요?\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# AI 메시지 추가\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\langchain_redis\\chat_message_history.py:120\u001b[39m, in \u001b[36mRedisChatMessageHistory.__init__\u001b[39m\u001b[34m(self, session_id, redis_url, key_prefix, ttl, index_name, redis_client, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mself\u001b[39m.redis_client.pubsub_configs = {\u001b[33m\"\u001b[39m\u001b[33mpush_handler_func\u001b[39m\u001b[33m\"\u001b[39m: _noop_push_handler}\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28mself\u001b[39m.redis_client.client_setinfo(\u001b[33m\"\u001b[39m\u001b[33mLIB-NAME\u001b[39m\u001b[33m\"\u001b[39m, __full_lib_name__)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ResponseError:\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# Fall back to a simple log echo\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m.redis_client.echo(__full_lib_name__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\commands\\core.py:709\u001b[39m, in \u001b[36mManagementCommands.client_setinfo\u001b[39m\u001b[34m(self, attr, value, **kwargs)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclient_setinfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m, value: \u001b[38;5;28mstr\u001b[39m, **kwargs) -> ResponseT:\n\u001b[32m    705\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    706\u001b[39m \u001b[33;03m    Sets the current connection library name or version\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[33;03m    For mor information see https://redis.io/commands/client-setinfo\u001b[39;00m\n\u001b[32m    708\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute_command(\u001b[33m\"\u001b[39m\u001b[33mCLIENT SETINFO\u001b[39m\u001b[33m\"\u001b[39m, attr, value, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\client.py:605\u001b[39m, in \u001b[36mRedis.execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **options):\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._execute_command(*args, **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\client.py:611\u001b[39m, in \u001b[36mRedis._execute_command\u001b[39m\u001b[34m(self, *args, **options)\u001b[39m\n\u001b[32m    609\u001b[39m pool = \u001b[38;5;28mself\u001b[39m.connection_pool\n\u001b[32m    610\u001b[39m command_name = args[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m conn = \u001b[38;5;28mself\u001b[39m.connection \u001b[38;5;129;01mor\u001b[39;00m pool.get_connection()\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._single_connection_client:\n\u001b[32m    614\u001b[39m     \u001b[38;5;28mself\u001b[39m.single_connection_lock.acquire()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\utils.py:183\u001b[39m, in \u001b[36mdeprecated_args.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m provided_args:\n\u001b[32m    179\u001b[39m         warn_deprecated_arg_usage(\n\u001b[32m    180\u001b[39m             arg, func.\u001b[34m__name__\u001b[39m, reason, version, stacklevel=\u001b[32m3\u001b[39m\n\u001b[32m    181\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\connection.py:1483\u001b[39m, in \u001b[36mConnectionPool.get_connection\u001b[39m\u001b[34m(self, command_name, *keys, **options)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28mself\u001b[39m._in_use_connections.add(connection)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1482\u001b[39m     \u001b[38;5;66;03m# ensure this connection is connected to Redis\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1483\u001b[39m     connection.connect()\n\u001b[32m   1484\u001b[39m     \u001b[38;5;66;03m# connections that the pool provides should be ready to send\u001b[39;00m\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# a command. if not, the connection was either returned to the\u001b[39;00m\n\u001b[32m   1486\u001b[39m     \u001b[38;5;66;03m# pool before all data has been read or the socket has been\u001b[39;00m\n\u001b[32m   1487\u001b[39m     \u001b[38;5;66;03m# closed. either way, reconnect and verify everything is good.\u001b[39;00m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\treze\\miniconda3\\envs\\llm\\Lib\\site-packages\\redis\\connection.py:384\u001b[39m, in \u001b[36mAbstractConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTimeout connecting to server\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m._error_message(e))\n\u001b[32m    386\u001b[39m \u001b[38;5;28mself\u001b[39m._sock = sock\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mConnectionError\u001b[39m: Error 10061 connecting to localhost:6379. 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다."
     ]
    }
   ],
   "source": [
    "# Radis 기반 채팅 기록 저장소\n",
    "from langchain_redis import RedisChatMessageHistory\n",
    "import os\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\") # 환경변수에서 Redis 호스트 정보 가져오기\n",
    "session_id = 'user_123'\n",
    "history = RedisChatMessageHistory(session_id=session_id, redis_url=REDIS_URL)\n",
    "history.add_user_message(\"안녕하세요 제 이름은 홍길동입니다.\") # 사용자 메시지 추가\n",
    "history.add_ai_message(\"안녕하세요 홍길동님, 무엇을 도와드릴까요?\") # AI 메시지 추가\n",
    "# 현재까지의 대화 내용 확인\n",
    "for msg in history.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\") # 메시지 타입과 내용 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c1721a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션기반 다중 사용자 메모리 구조 구현 - 다중 사용자 챗봇\n",
    "# 핵심 : session_id를 키로 하는 메모리 저장소를 만들고 사용자의 대화는 키별로 저장한다.\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 프롬프트\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 뛰어난 한국어 상담 챗봇입니다. 질문에 친절하고 자세히 답변해주세요.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), # 대화 기록을 위한 플레이스홀더\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) # LLM 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "babcb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser() # 문자열 출력 파서 객체 생성\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66c4e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션별 메모리 저장소를 딕셔너리로 만들고, 존재하지 않는 새로운 세션 id가 들어오면 InMEmoryChatMessageHistory 객체를 생성\n",
    "# get_session_history를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9ac63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# 세션 id -> 대화 기록 객체 매핑\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    '''\n",
    "    세션 ID에 해당하는 대화 기록을 반환합니다.\n",
    "    만약 세션 ID가 존재하지 않으면 새로운 InMemoryChatMessageHistory 객체를 생성하여 반환합니다.\n",
    "    '''\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 메모리를 통합한 체인 래퍼 생성\n",
    "chatbot = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history, # 세션별 대화 기록을 가져오는 함수\n",
    "    input_messages_key=\"input\", # 사용자 입력 키\n",
    "    history_messages_key=\"history\" # LLM 응답 키\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:34:08 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Session user_a 질문: 안녕하세요, 저는 홍길동입니다. 당신은 누구신가요?\n",
      "Session user_a 응답: 안녕하세요, 홍길동님! 저는 여러분의 질문에 답변하고 도움을 드리기 위해 만들어진 챗봇입니다. 어떤 궁금한 점이나 도움이 필요하신 부분이 있으신가요?\n",
      "\n",
      "16:34:10 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Session user_b 질문: 안녕하세요, 저는 이순신입니다. 당신은 어떤 일을 하시나요?\n",
      "Session user_b 응답: 안녕하세요, 이순신님! 저는 여러분의 질문에 답변하고, 다양한 정보와 도움을 제공하는 챗봇입니다. 궁금한 점이나 도움이 필요한 부분이 있다면 언제든지 말씀해 주세요!\n",
      "\n",
      "16:34:12 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Session user_a 질문: 저는 프로그래밍을 배우고 있습니다. 당신은 어떤 일을 하시나요?\n",
      "Session user_a 응답: 프로그래밍을 배우고 계시다니 멋지네요! 저는 여러분의 질문에 답변하고, 정보 제공, 문제 해결, 그리고 다양한 주제에 대한 상담을 하는 역할을 하고 있습니다. 프로그래밍에 관련된 질문이나 도움이 필요하시면 언제든지 말씀해 주세요! 어떤 언어를 배우고 계신가요?\n",
      "\n",
      "16:34:14 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Session user_b 질문: 저는 역사에 관심이 많습니다. 당신은 어떤 분야에 관심이 있나요?\n",
      "Session user_b 응답: 역사에 관심이 많으시다니 정말 멋지네요! 저는 특정한 관심 분야가 없지만, 다양한 주제에 대한 정보를 제공할 수 있습니다. 역사, 과학, 기술, 문화, 예술 등 여러 분야에 대해 이야기할 수 있으니, 궁금한 점이나 더 알고 싶은 주제가 있다면 말씀해 주세요!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 두개의 세션을 번갈아가면서 대화\n",
    "sessions = [\"user_a\", \"user_b\"]\n",
    "questions = [\n",
    "    \"안녕하세요, 저는 홍길동입니다. 당신은 누구신가요?\", # user_a 첫번째 질문\n",
    "    \"안녕하세요, 저는 이순신입니다. 당신은 어떤 일을 하시나요?\", # user_b 첫번째 질문\n",
    "    \"저는 프로그래밍을 배우고 있습니다. 당신은 어떤 일을 하시나요?\", # user_a 두번째 질문\n",
    "    \"저는 역사에 관심이 많습니다. 당신은 어떤 분야에 관심이 있나요?\" # user_b 두번째 질문\n",
    "]\n",
    "# 세션별로 질문을 번갈아가며 대화\n",
    "for i, question in enumerate(questions):\n",
    "    session_id = sessions[i % len(sessions)] # 세션 ID 선택\n",
    "    result = chatbot.invoke({\"input\" : question}, config={'configurable': {'session_id': session_id}}) # 세션 ID를 설정하여 invoke\n",
    "    print(f\"Session {session_id} 질문: {question}\") \n",
    "    print(f\"Session {session_id} 응답: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be2e3b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:42:18 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Session user_c 질문: 안녕하세요. 저는 철수에요. 반갑습니다.\n",
      "Session user_c 응답: 안녕하세요, 철수님! 다시 만나서 반갑습니다. 오늘은 어떤 이야기를 나누고 싶으신가요? 궁금한 점이나 도움이 필요한 부분이 있다면 말씀해 주세요!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = chatbot.invoke({\"input\" : \"안녕하세요. 저는 철수에요. 반갑습니다.\"}, config={'configurable': {'session_id': \"user_c\"}}) # 새로운 세션 user_c\n",
    "print(f\"Session user_c 질문: 안녕하세요. 저는 철수에요. 반갑습니다.\")\n",
    "print(f\"Session user_c 응답: {result}\\n\") # 새로운 세션 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9054a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:43:18 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Session user_a 질문: 안녕하세요. 저는 누구라구요?\n",
      "Session user_a 응답: 안녕하세요! 홍길동님이라고 하셨습니다. 혹시 다른 질문이나 궁금한 점이 있으신가요? 도움이 필요하시면 언제든지 말씀해 주세요!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = chatbot.invoke({\"input\" : \"안녕하세요. 저는 누구라구요?\"}, config={'configurable': {'session_id': \"user_a\"}})\n",
    "print(f\"Session user_a 질문: 안녕하세요. 저는 누구라구요?\")\n",
    "print(f\"Session user_a 응답: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dacd1ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:44:01 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Session user_c 질문: 안녕하세요. 저는 누구라구요?\n",
      "Session user_c 응답: 안녕하세요! 당신은 철수라고 말씀하셨습니다. 혹시 다른 질문이나 궁금한 점이 있으신가요? 도와드릴 수 있는 것이 있다면 말씀해 주세요!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = chatbot.invoke({\"input\" : \"안녕하세요. 저는 누구라구요?\"}, config={'configurable': {'session_id': \"user_c\"}}) \n",
    "print(f\"Session user_c 질문: 안녕하세요. 저는 누구라구요?\")\n",
    "print(f\"Session user_c 응답: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b0903",
   "metadata": {},
   "source": [
    "### 요약 메모리 구현(대화 내용 자동 요약)\n",
    "- 긴 대화내용을 모두 프롬프트에 기록하는 것은 비효율적 -> 프롬프트 길이 제한에 걸릴 수 있음음\n",
    "\n",
    "### Conversation Summary Memory\n",
    "- 0.3x 버전에서는 직접 요약용 체인을 만들어서 ChatMemoryHistory에 저장\n",
    "- 어떻게 요약?\n",
    "    - 일정길이 이상으로 대화가 누적되면, 과거 대화를 요약해서 핵심내용만 남김\n",
    "    - 요약결과를 메모리에 시스템 메세지 등으로 저장->메모리 절약\n",
    "    - 새로운 사용자 입력시 요약된 맥락 + 최근 몇 메시지만 참고해서 llm 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c8518e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약용 프롬프트 템플릿\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 뛰어난 요약 전문가입니다. 주어진 대화를 간결하게 요약해주세요.\"),\n",
    "    (\"human\", \"{conversation}\") # 전체 대화 내용을 하나의 문자열로전달\n",
    "])\n",
    "#LCEL\n",
    "summary_chain = summary_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f6d8b5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:09:08 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "18:09:11 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "18:09:13 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "18:09:17 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "18:09:21 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "요약 전 user_d의 메모리 메세지 개수 : 23\n",
      "[SystemMessage(content='요약: HUMAN은 내일 회의 자료를 준비해야 한다고 언급하며 회의 시간과 참석자를 확인하려고 한다. AI는 회의 일정과 참석자 목록은 이메일이나 캘린더에서 확인할 수 있다고 조언하고, 단위 프로젝트 진행 상황을 공유하는 것이 좋다고 제안한다. AI는 공유할 내용으로 현재 진행 상황, 문제점 및 해결 방안, 다음 단계, 필요한 지원 등을 포함할 것을 추천한다.', additional_kwargs={}, response_metadata={}), HumanMessage(content='최근에 이야기했던 새로운 기능에 대한 업데이트는 있어?', additional_kwargs={}, response_metadata={}), AIMessage(content='새로운 기능에 대한 업데이트는 보통 프로젝트 팀이나 개발 팀에서 관리하고 있습니다. 만약 그 기능에 대한 구체적인 정보가 필요하시다면, 관련 팀이나 담당자에게 직접 문의해보는 것이 가장 확실합니다. \\n\\n회의에서 이 업데이트를 공유할 계획이라면, 다음과 같은 내용을 포함하면 좋습니다:\\n\\n1. **기능 설명**: 새로운 기능이 무엇인지 간단히 설명합니다.\\n2. **진행 상황**: 현재 개발 상태나 테스트 진행 여부.\\n3. **출시 일정**: 기능이 언제 출시될 예정인지.\\n4. **기대 효과**: 이 기능이 사용자나 프로젝트에 어떤 긍정적인 영향을 미칠 것인지.\\n\\n이런 정보를 준비하면 회의에서 유용하게 활용할 수 있습니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={}), HumanMessage(content='안녕, 오늘 우리 뭐하려고 했지?', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕하세요! 오늘 계획했던 내용을 확인하려면, 이전에 나눈 대화나 회의록을 참고하는 것이 좋습니다. 만약 구체적인 일정이나 주제가 있다면, 그에 대해 말씀해 주시면 더 자세히 도와드릴 수 있습니다. \\n\\n예를 들어, 오늘 회의가 예정되어 있다면, 회의의 주제나 참석자, 준비해야 할 자료 등을 확인하는 것이 중요합니다. 어떤 내용을 이야기하고 싶으신지 알려주시면, 더 구체적으로 도와드릴 수 있습니다!', additional_kwargs={}, response_metadata={}), HumanMessage(content='아 맞다 내일 회의 자료 준비해야지, 회의는 몇시지?', additional_kwargs={}, response_metadata={}), AIMessage(content='회의 시간이 궁금하시다면, 이메일이나 캘린더에서 확인하시는 것이 가장 좋습니다. 보통 회의 일정은 이러한 플랫폼에 기록되어 있으니, 확인해 보시면 정확한 시간을 알 수 있을 거예요.\\n\\n또한, 회의 자료를 준비할 때는 다음과 같은 내용을 포함하는 것이 좋습니다:\\n\\n1. **현재 진행 상황**: 프로젝트나 주제에 대한 최신 정보.\\n2. **문제점 및 해결 방안**: 현재 직면하고 있는 문제와 그에 대한 해결책.\\n3. **다음 단계**: 앞으로의 계획이나 일정.\\n4. **필요한 지원**: 추가로 필요한 자원이나 지원 사항.\\n\\n이런 내용을 포함하면 회의에서 유용하게 활용할 수 있습니다. 추가로 도움이 필요하시면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={}), HumanMessage(content='그 회의는 누가 참석하는지 기억나니?', additional_kwargs={}, response_metadata={}), AIMessage(content='회의 참석자 목록은 보통 이메일 초대장이나 캘린더 이벤트에 포함되어 있습니다. 그곳에서 참석자들을 확인하실 수 있을 거예요. \\n\\n만약 참석자 목록을 확인할 수 없다면, 이전 회의록이나 관련 문서에서 확인해 보시는 것도 좋은 방법입니다. 참석자들이 누구인지 아는 것은 회의 준비에 매우 중요하니, 꼭 확인해 보세요!\\n\\n더 궁금한 점이나 도움이 필요하시면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={}), HumanMessage(content='단위 프로젝트 진행 상황도 공유해야 할까?', additional_kwargs={}, response_metadata={}), AIMessage(content='네, 단위 프로젝트 진행 상황을 공유하는 것은 매우 중요합니다. 회의에서 진행 상황을 공유하면 참석자들이 현재 상태를 이해하고, 문제를 함께 논의하며, 다음 단계에 대한 계획을 세우는 데 도움이 됩니다. \\n\\n공유할 내용으로는 다음과 같은 항목을 포함하는 것이 좋습니다:\\n\\n1. **현재 진행 상황**: 프로젝트의 어떤 부분이 완료되었고, 어떤 부분이 진행 중인지 설명합니다.\\n2. **문제점 및 해결 방안**: 현재 직면하고 있는 문제와 그에 대한 해결책을 제시합니다.\\n3. **다음 단계**: 앞으로의 계획이나 일정에 대해 논의합니다.\\n4. **필요한 지원**: 프로젝트 진행을 위해 필요한 자원이나 지원 사항을 요청합니다.\\n\\n이런 정보를 준비하면 회의에서 더 효과적으로 소통할 수 있습니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={}), HumanMessage(content='최근에 이야기했던 새로운 기능에 대한 업데이트는 있어?', additional_kwargs={}, response_metadata={}), AIMessage(content='새로운 기능에 대한 업데이트는 보통 개발 팀이나 프로젝트 팀에서 관리하고 있습니다. 만약 그 기능에 대한 구체적인 정보가 필요하시다면, 관련 팀이나 담당자에게 직접 문의해보는 것이 가장 확실합니다.\\n\\n회의에서 이 업데이트를 공유할 계획이라면, 다음과 같은 내용을 포함하면 좋습니다:\\n\\n1. **기능 설명**: 새로운 기능이 무엇인지 간단히 설명합니다.\\n2. **진행 상황**: 현재 개발 상태나 테스트 진행 여부.\\n3. **출시 일정**: 기능이 언제 출시될 예정인지.\\n4. **기대 효과**: 이 기능이 사용자나 프로젝트에 어떤 긍정적인 영향을 미칠 것인지.\\n\\n이런 정보를 준비하면 회의에서 유용하게 활용할 수 있습니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={}), HumanMessage(content='안녕, 오늘 우리 뭐하려고 했지?', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕하세요! 오늘의 계획을 확인하려면, 이전에 나눈 대화나 회의록을 참고하는 것이 좋습니다. 만약 구체적인 일정이나 주제가 있다면, 그에 대해 말씀해 주시면 더 자세히 도와드릴 수 있습니다.\\n\\n예를 들어, 오늘 회의가 예정되어 있다면, 회의의 주제나 참석자, 준비해야 할 자료 등을 확인하는 것이 중요합니다. 어떤 내용을 이야기하고 싶으신지 알려주시면, 더 구체적으로 도와드릴 수 있습니다!', additional_kwargs={}, response_metadata={}), HumanMessage(content='아 맞다 내일 회의 자료 준비해야지, 회의는 몇시지?', additional_kwargs={}, response_metadata={}), AIMessage(content='회의 시간이 궁금하시다면, 이메일이나 캘린더에서 확인하시는 것이 가장 좋습니다. 보통 회의 일정은 이러한 플랫폼에 기록되어 있으니, 확인해 보시면 정확한 시간을 알 수 있을 거예요.\\n\\n회의 자료를 준비할 때는 다음과 같은 내용을 포함하는 것이 좋습니다:\\n\\n1. **현재 진행 상황**: 프로젝트나 주제에 대한 최신 정보.\\n2. **문제점 및 해결 방안**: 현재 직면하고 있는 문제와 그에 대한 해결책.\\n3. **다음 단계**: 앞으로의 계획이나 일정.\\n4. **필요한 지원**: 추가로 필요한 자원이나 지원 사항.\\n\\n이런 내용을 포함하면 회의에서 유용하게 활용할 수 있습니다. 추가로 도움이 필요하시면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={}), HumanMessage(content='그 회의는 누가 참석하는지 기억나니?', additional_kwargs={}, response_metadata={}), AIMessage(content='회의 참석자 목록은 보통 이메일 초대장이나 캘린더 이벤트에 포함되어 있습니다. 그곳에서 참석자들을 확인하실 수 있을 거예요. \\n\\n만약 참석자 목록을 확인할 수 없다면, 이전 회의록이나 관련 문서에서 확인해 보시는 것도 좋은 방법입니다. 참석자들이 누구인지 아는 것은 회의 준비에 매우 중요하니, 꼭 확인해 보세요!\\n\\n더 궁금한 점이나 도움이 필요하시면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={}), HumanMessage(content='단위 프로젝트 진행 상황도 공유해야 할까?', additional_kwargs={}, response_metadata={}), AIMessage(content='네, 단위 프로젝트 진행 상황을 공유하는 것은 매우 중요합니다. 회의에서 진행 상황을 공유하면 참석자들이 현재 상태를 이해하고, 문제를 함께 논의하며, 다음 단계에 대한 계획을 세우는 데 도움이 됩니다.\\n\\n공유할 내용으로는 다음과 같은 항목을 포함하는 것이 좋습니다:\\n\\n1. **현재 진행 상황**: 프로젝트의 어떤 부분이 완료되었고, 어떤 부분이 진행 중인지 설명합니다.\\n2. **문제점 및 해결 방안**: 현재 직면하고 있는 문제와 그에 대한 해결책을 제시합니다.\\n3. **다음 단계**: 앞으로의 계획이나 일정에 대해 논의합니다.\\n4. **필요한 지원**: 프로젝트 진행을 위해 필요한 자원이나 지원 사항을 요청합니다.\\n\\n이런 정보를 준비하면 회의에서 더 효과적으로 소통할 수 있습니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={}), HumanMessage(content='최근에 이야기했던 새로운 기능에 대한 업데이트는 있어?', additional_kwargs={}, response_metadata={}), AIMessage(content='새로운 기능에 대한 업데이트는 보통 개발 팀이나 프로젝트 팀에서 관리하고 있습니다. 만약 그 기능에 대한 구체적인 정보가 필요하시다면, 관련 팀이나 담당자에게 직접 문의해보는 것이 가장 확실합니다.\\n\\n회의에서 이 업데이트를 공유할 계획이라면, 다음과 같은 내용을 포함하면 좋습니다:\\n\\n1. **기능 설명**: 새로운 기능이 무엇인지 간단히 설명합니다.\\n2. **진행 상황**: 현재 개발 상태나 테스트 진행 여부.\\n3. **출시 일정**: 기능이 언제 출시될 예정인지.\\n4. **기대 효과**: 이 기능이 사용자나 프로젝트에 어떤 긍정적인 영향을 미칠 것인지.\\n\\n이런 정보를 준비하면 회의에서 유용하게 활용할 수 있습니다. 추가로 궁금한 점이 있으면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# user_d 세션에 대화 내용을 기록. 긴 대화 생성\n",
    "long_queries = [\n",
    "    \"안녕, 오늘 우리 뭐하려고 했지?\",\n",
    "    \"아 맞다 내일 회의 자료 준비해야지, 회의는 몇시지?\",\n",
    "    \"그 회의는 누가 참석하는지 기억나니?\",\n",
    "    \"단위 프로젝트 진행 상황도 공유해야 할까?\",\n",
    "    \"최근에 이야기했던 새로운 기능에 대한 업데이트는 있어?\"\n",
    "]\n",
    "session_id = \"user_d\"\n",
    "for q in long_queries:\n",
    "    result = chatbot.invoke({\"input\": q}, config={'configurable': {'session_id': session_id}})\n",
    "\n",
    "print(f'요약 전 user_d의 메모리 메세지 개수 : {len(get_session_history(session_id).messages)}') # 메모리 메시지 개수 출력\n",
    "print(store[session_id].messages) # 메모리 메시지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8c94b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:09:24 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "요약 결과: HUMAN은 내일 회의 자료를 준비해야 하며, 회의 시간과 참석자를 확인하려고 한다. AI는 이메일이나 캘린더에서 확인할 것을 권장하고, 회의 자료에 포함할 내용으로 현재 진행 상황, 문제점 및 해결 방안, 다음 단계, 필요한 지원 등을 제안한다. 또한, 새로운 기능에 대한 업데이트는 관련 팀에 문의하라고 조언하며, 회의에서 공유할 내용으로 기능 설명, 진행 상황, 출시 일정, 기대 효과를 포함할 것을 추천한다.\n"
     ]
    }
   ],
   "source": [
    "# 전체 대화내용을 요약하고 마지막 사용자 질문-답변 쌍만 원본 유지\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "# 요약 대상 대화내용 추출(마지막 QA 쌍 제외한 이전 내용)\n",
    "message = store[session_id].messages\n",
    "\n",
    "if len(message) > 2:\n",
    "    original_dialogue = '\\n'.join([f'{msg.type.upper()} : {msg.content}' for msg in message[:-2]])\n",
    "else :\n",
    "    original_dialogue = '\\n'.join([f'{msg.type.upper()} : {msg.content}' for msg in message])\n",
    "\n",
    "# llm으로 요약 생성\n",
    "summary_text = summary_chain.invoke({'conversation' : original_dialogue}) # 대화 내용과 이전 대화 기록을 전달\n",
    "print(f\"요약 결과: {summary_text}\") # 요약 결과 출력\n",
    "\n",
    "# 기존 메모리를 요약으로 교체 : 이전 내용 요약본 + 최근 QA 유지\n",
    "new_history = InMemoryChatMessageHistory()\n",
    "new_history.messages.append(SystemMessage(content=f\"요약: {summary_text}\")) # 요약 메시지 추가\n",
    "# 최근 대화의 마지막 QA 쌍 복원\n",
    "if len(message) >= 2:\n",
    "    last_user_message = message[-2]\n",
    "    last_ai_message = message[-1]\n",
    "    if isinstance(last_user_message, HumanMessage):\n",
    "        new_history.add_user_message(last_user_message.content)\n",
    "    else : \n",
    "        new_history.messages.append(last_user_message) # 사용자 메시지 추가\n",
    "    if isinstance(last_user_message, AIMessage):\n",
    "        new_history.add_user_message(last_ai_message.content)\n",
    "    else : \n",
    "        new_history.messages.append(last_ai_message) # 사용자 메시지 추가\n",
    "\n",
    "# 메모리 교체\n",
    "store[session_id] = new_history # 새로운 메모리로 교체\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5132f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
